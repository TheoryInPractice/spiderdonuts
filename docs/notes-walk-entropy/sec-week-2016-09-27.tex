\subsection{current questions}

Returning to a point from last week: we are trying to get at the following:

> is the problem we are studying really about infinite sequences (power series coefficients), or can it always be reduced to looking for a property in the first $n$ terms $WR(\diag(\mA^k))$ for $k = 1:n$?

(or some finite constant determined by $n$?).

\textbf{Claim:} Yes!

\begin{lemma}[Finite walks]\label{thm:finitewalks}
  Let graph $G$ have walk-vectors $\vd_j = \diag(\mA^j)$, and let $\mA$ have minimial polynomial of degree $m$. Let $\vb$ be any positive vector of appropriate dimension.
  If there exists a set of positive coefficients $x_j > 0 $ for $j = 0:m$ such that $WR(\sum_{j=0}^{m}  x_j \vd_j )=  \vb$, then there exists a positive power-series function $f(x)$ such that $WR(\diag( f(\mA))) = \vb$
\end{lemma}
\begin{proof}
By assumption we have found $\vx > 0 $ such that $\mW\vx =  \vb$. We will construct the appropriate function using the values $x_j > 0$.

Recall that the degree of the minimal polynomial of $\mA$ is $m$.
First we argue, using properties of the minimal polynomial of any $n\times n$ matrix, that for each index $k = (m+1) :\infty$ there exists a set of coefficients $\{ p_{k,j} \}_{j=0}^{m}$ such that
\[
\mA^k = \sum_{j=0}^{m} p_{k,j} \mA^j.
\]
This is important because it will enable us to collapse all terms $\mA^k$ for $k=(m+1):\infty$ into just the first $m$ terms $\{\mA^j\}_{j=0}^m$.
Before we can perform this collapsing, we first need to adjust the coefficients $p_{k,j}$ can be put into certain convergent sums. The motivation for this will become clear in a couple paragraphs.

For each $j$, construct a sequence $\{s_{j,k}\}_{k=m+1}^{\infty}$ so that $\left(\sum_{k=m+1}^{\infty} p_{k,j} s_{j,k} \right)$ converges to some positive value $c_j$. One example: $s_{j,k} := 2^{-k} |p_{k,j}^{-1}|$; if $p_{k,j} = 0$ then instead use $2^{-k}$. Note that $s_{j,k}$ is positive!

Next, for each $k$ define $c_k = \displaystyle \min_{j=0:m} s_{j,k}$. Then we have that $\left( \sum_{k=m+1}^{\infty} p_{k,j} c_k \right)$ converges, for each $j$. Using the example of the coefficients $s_{j,k}$ we defined above, we have $|p_{k,j}c_k| \leq 2^{-k}$, and so the magnitude of the sum $\sum_{k=m+1}^{\infty} p_{k,j} c_k$ is bounded above by $\sum_{k=m+1}^{\infty} 2^{-k}$.

Finally, for $j=0:m$, choose $c_j = x_j - \left( \sum_{k=m+1}^{\infty} p_{k,j} c_k \right)$. If any of these $c_j$ is negative, then choose $\{ c_k \}_{k=m+1}^{\infty}$ smaller so that the values $c_j = x_j - \left( \sum_{k=m+1}^{\infty} p_{k,j} c_k \right)$ are positive.
We know this is possible because the $c_k$ can be chosen as close to 0 as we like, and $x_j > 0$ by assumption; thus the difference $x_j - \left( \sum_{k=m+1}^{\infty} p_{k,j} c_k \right)$ can be made positive by choosing $c_k$ small enough.
Choosing smaller values for $c_k$ cannot cause the sums $\sum_{k=m+1}^{\infty} p_{k,j} c_k$ to fail to converge because smaller values of $c_k$ can only cause the sum to converge more rapidly.

The end result is that any positive soution $\vx$ to $\mW\vx = \vb$ enables us to construct
\begin{align}
  \sum_{j=0}^{m} x_j \vd_{j} &=  \vb \\
  \sum_{j=0}^{m} x_j \diag(\mA^j) &=  \vb \\
  \diag\left( \sum_{j=0}^{m} x_j \mA^j \right) &=  \vb \\
  \diag\left( \sum_{j=0}^{m} \left( c_j +  \sum_{k=m+1}^{\infty} p_{k,j} c_k \right) \mA^j \right) &=  \vb.
\end{align}
Rearranging the summations yields
\begin{align}
   \vb &=  \diag\left( \sum_{j=0}^{m} c_j \mA^j  + \sum_{j=0}^m \left( \sum_{k=m+1}^{\infty} p_{k,j} c_k \right) \mA^j \right) \\
  &= \diag\left( \sum_{j=0}^{m} c_j \mA^j  +  \left( \sum_{k=m+1}^{\infty} c_k \sum_{j=0}^m p_{k,j}  \mA^j \right)  \right) \\
  &= \diag\left( \sum_{j=0}^{m} c_j \mA^j  +  \left( \sum_{k=m+1}^{\infty} c_k \mA^k \right)  \right) \\
  &= \diag \left( \sum_{j=0}^{\infty} c_j \mA^j \right).
\end{align}
This completes a proof that we have constructed coefficients for a positive power-series function $f(x)$ so that $WR(\diag(f(\mA))) = \vb$, as desired.
\end{proof}

\paragraph{Remarks:}

It is not clear that this proof says anything about whether or not the existence of a deceptive function implies the existence of a positive solution $\vx$ to the equation $\mM\vx = \vb$.

However, the machinery used in the above proof \emph{does} demonstrate that a deceptive function guarantees \emph{some} solution must exist to the equation $\mM\vx = \ve$ -- but the $\vx$ produced might not be positive (!?).

Therefore: if $\mM\vx = \ve$ has \emph{no} solutions, then there are definitely not deceptive functions, i.e. $G$ is honest.

\begin{corollary}
  If $\mM\vx = \ve$ has no solutions (of any kind, not just positive solutions), then there are no deceptive functions for the underlying graph $G$, i.e. $G$ is honest.
\end{corollary}





\paragraph{Can we combine this with earlier results?}
Now we can combine this with the ``Farkas Lemma" Lemma!

Let $g(x)$ be any positive power-series function and set $\vg = WR(\diag(g(\mA)))$ and choose $\gamma$ so that $\gamma\ve - \vg$ is positive. By Lemma~\ref{thm:farkaslemmalemma} we know that any nonnegative solution to $\mM \vy = \gamma \ve -\vg$ can be turned into a positive solution $\vx$ to a slightly different equation $\mM\vx = \gamma - \vg'$ still with positive right-hand side.
By Lemma~\ref{thm:finitewalks} we now know that any positive solution $\vx$ to an equation of the form $\mM\vx = \vb$ with positive $\vb$ yields a positive power-series function $h(x)$ such that $WR( \diag (h(\mA)) ) = \vb$.


%I DON'T THINK THIS WORKS:
%
%Now, suppose there exists a deceptive function $H(x)$ so that $\vh = WR(\diag(H(\mA))) = \alpha \ve$. Scale the function so that $\alpha < 1$, then choose $\gamma = 1 - \alpha$.
%By Lemma~\ref{thm:farkaslemmalemma} we have that any nonnegative solution to $\mM\vy = \gamma\ve + \alpha\ve \ve = \ve$ will yield a positive solution to a slightly different equation $\mM\vx = \gamma\ve - \vh$; and by Lemma~\ref{thm:finitewalks} we know that any positive solution $\vx$ to $\mM\vx = \ve$ yields a deceptive function.

%Therefore, if there exists a deceptive function, then a nonnegative solution to $\mM\vx = \ve$ can construct a deceptive function. ?



\paragraph{Necessary condition for deceptiveness}

Recall the \emph{Average-Condition Flip-Flop}: we say that any matrix $\mM$ satisfies ACFF if for every pair of disjoint, non-empty subsets $S$ and $T$, there exists a column $j$ such that $\ave(\mM(T,j)) \leq \ave(\mM(S,j))$.
\begin{lemma}
  The Average-Condition Flip-Flop is a \emph{necessary} condition for the existence of a nonnegative solution to $\mM\vx = \ve$ (where $\mM$ is any matrix and $\ve$ is the vector of all 1s).
\end{lemma}
\begin{proof}
  Assume that $\mM$ does not satisfy ACFF. We will construct a vector $\vy$ such that $\vy^T\mM \geq 0$ but $\vy^T\ve < 0$, which would imply via Farkas's Lemma that there is no $\vx \geq 0$ such that $\mM\vx = \ve$.


Since we have assumed $\mM$ does not satisfy ACFF, there must exist disjoint, non-empty subsets $S,T$ such that for all columns $j$ we have $\ave(\mM(T,j)) \lneq \ave(\mM(S,j))$.

  Construct the vector $\vy$ as follows. Set $\vy(S) = 1/|1|$, and set $\vy(T) = -(1+\delta)/|T|$ for any $\delta > 0$.
  Then $\vy = \tfrac{1}{|S|}\ve_S - \tfrac{1+\delta}{|T|}\ve_T$, and we have
  \[
    \vy^T\ve = 1 - (1+\delta) = -\delta < 0
  \]
  as long as $\delta > 0$.

  Next we pick a specific $\delta > 0$ so that $\vy$ satisfies $\vy^T\mM \geq 0$.
  Note that by construction of $S$ and $T$, we know $\ave(\mM(T,j)) \lneq \ave(\mM(S,j))$ for all $j$.
  Thus, we know the quantity
  \begin{equation}
    \delta := \displaystyle\min_j \left(  \frac{\ave(\mM(S,j)) - \ave(\mM(T,j))}{\ave(\mM(T,j))}   \right)
  \end{equation}
  is positive, since   $\ave(\mM(S,j)) - \ave(\mM(T,j)) > 0$ follows from the fact that $\ave(\mM(T,j)) \lneq \ave(\mM(S,j))$ for all $j$.

  Finally, we will show that an arbitrary column, $j$, of the row vector $\vy^T\mM$ is nonnegative (i.e. $\vy^T\mM \geq 0$).
  Expanding $\vy = \tfrac{1}{|S|}\ve_S - \tfrac{1+\delta}{|T|}\ve_T$ and multiplying with column $j$ of $\mM$ we get
  \begin{align}
    (\vy^T\mM)_j &= \left(\tfrac{1}{|S|}\ve_S - \tfrac{1+\delta}{|T|}\ve_T\right)\mM(:,j) \\
    &= \ave(\mM(S,j)) - (1+\delta)\ave(\mM(T,j)) \\
    &= \ave(\mM(S,j)) - \ave(\mM(T,j)) - \delta \ave(\mM(T,j)).
  \end{align}
  By construction of $\delta$, we know that
  \begin{align}
    \delta \ave(\mM(T,j)) &=  \ave(\mM(T,j))  \displaystyle\min_i \left(  \frac{\ave(\mM(S,i)) - \ave(\mM(T,i))}{\ave(\mM(T,i))}   \right) \\
    &\leq \ave(\mM(T,j))  \left(  \frac{\ave(\mM(S,j)) - \ave(\mM(T,j))}{\ave(\mM(T,j))}   \right) \\
    &=  \ave(\mM(S,j)) - \ave(\mM(T,j))
  \end{align}
  which is always positive for all $j$ by our assumption that $\ave(\mM(S,j)) > \ave(\mM(T,j))$ for all $j$. Now since we have constructed $\delta$ so that
  $\delta \ave(\mM(T,j)) \leq   \ave(\mM(S,j)) - \ave(\mM(T,j))$ holds,
we can upperbound our string of equalities above:
\begin{align}
    (\vy^T\mM)_j &= \ave(\mM(S,j)) - \ave(\mM(T,j)) - \delta \ave(\mM(T,j)) \\
    & \geq \ave(\mM(S,j)) - \ave(\mM(T,j)) - ( \ave(\mM(S,j)) - \ave(\mM(T,j))  ) \\
    &= 0
\end{align}
proving that $(\vy^T\mM)_j \geq 0 $ for all $j$.

Thus, by Farkas's Lemma, no solution $\vx \geq 0 $ can exist to the equation $\mM\vx = \ve$.
This completes a proof that the Average-Condition Flip-Flop is necessary for the existence of $\vx \geq 0 $ solving $\mM\vx = \ve$.

\end{proof}


\paragraph{Current questions}
Can there exist deceptive functions that give rise to non-positive solutions $\vz$ to $\mM\vz = \ve$ ?
Is ``Existence of deceptive function" equivalent to ``Existence of a positive solution to $\mM\vx = \ve$" ?

We have proved one direction of this: positive-solution guarantees deceptive function.


\paragraph{Important point}

Eric just pointed out that Average Condition does *not* imply ECML as I had previously written.

DFF implies ECM and PWFF. But ACFF just implies PWFF. ACFF (and PWFF) are necessary for $\mM\vx = \ve$ to have solutions.



\section{Refining the Flip-Flop conditions}

The original statements of these FF conditions had some problems.
We try to correct things here and state the most up-to-date versions of the conditions, together with examples that show why each condition might be useful / sufficient / necessary. In some cases we have proof that a couple conditions are necessary.(No proofs yet that anything is sufficient.)


As mentioned during our first attempt at defining FF conditions, part of our design choice is motivated by the following matrix:
$\mM = \bmat{ 3 & 1 \\   2 & 2 \\ 1 & 3 }$.

This matrix *does* have an all positive solution, $\vx = [1;1]$, to the equation $\mM\vx = \ve$. Because of that, any Flip-Flop property that we want to be necessary for deceptiveness had better hold for this $\mM$ (or else $\mM$ is a counterexample to our property being necessary).

In particular, the Average Condition FF is satisfied in only the soft sense by this $\mM$. Thus, the Average Condition FF cannot use a strict inequality, or else this $\mM$ would serve as a counterexample to ACFF being necessary for deceptiveness.

\textbf{Conclusion}: Average Condition must be defined using soft-inequalities.

At the same time, using a soft-inequality for ACFF is a little problematic. Consider the following walk-class matrix for the pyrmiad-prism with 3 faces and 1 extra layer:
\begin{verbatim}
M2 = [ [3, 6, 24, 78, 279, 990, 3588, 13122, 48495, 180654]
       [4, 6, 37, 96, 432, 1378, 5581, 19608, 76412, 281278]
       [4, 2, 36, 66, 424, 1218, 5692, 19530, 81120, 298714] ]
\end{verbatim}
M2 is not deceptive, yet it satisfies Average Condition in the soft-sense!
However, this matrix does \emph{not} satisfy Pair-wise FF in the strict sense.
The reason we know this is not deceptive is because row 1 is always $\leq$ row 2 (entry-wise) -- this property is supposed to be ruled out by Pair-wise Flip-flopping, but I forgot about that when I switched PWFF to use soft inequalities.

\textbf{Conclusion}: ACFF uses soft inequalities, but PWFF uses strict inequalities.

ECM and DFF should be using strict inequalities, but I no longer believe those conditions will be very useful. I think our best bet is for a combination of ACFF(soft) and PWFF(strict) -- we already know they are both necessary, and what's more they are not equivalent, so they are ruling out different sets of non-deceptive matrices.
