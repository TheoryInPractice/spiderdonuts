\subsection{Eigen-fundamentals}\label{sec:fundamentals:eigen}

Eigen-information of graph-related matrices turns out to be a powerful tool in analyzing structural information of graphs. For example, we will see that the second smallest eigenvalue of the Laplacian matrix of a graph gives information about the best cut we can find in the graph. In a broader setting, the eigen-information of a matrix provides powerful tools for faster matrix operations and can even be used for data compression.


Let $\mM$ be any $n \times n$ matrix with entries over the real numbers $\mathbb{R}$. A vector $\vv$ is an \emph{eigenvector} of $\mM$ with \emph{eigenvalue} $\lam$ iff $\mM\vv = \lam \vv$. We call the tuple $(\vv, \lam)$ an \emph{eigen-pair}. Suppose the matrix $\mM$ has $n$ eigenvectors $\vv_j$ with eigenvalues $\lam_j$. Set $\mV = [\vv_1, \cdots, \vv_n]$ and $\mLam = \diag(\lam_1, \cdots, \lam_n)$. Then we have that the equation
\[
\mM\mV = \mV\mLam
\]
holds column-wise by definition of the columns $\vv_j$ being eigenvectors of $\mM$ (verify this). Note that the matrix $\mV$ is invertible iff the $n$ eigenvectors $\vv_j$ are linearly independent; if they are, we can then rearrange the above equation to yield the \emph{diagonalization} of matrix $\mM$, also called the \emph{spectral decomposition} or \emph{eigen decomposition} of matrix $\mM$:
\begin{equation}\label{eqn:spectraldecomposition}
\mM = \mV \mLam \mV\inv.
\end{equation}
If a matrix $\mM$ has a diagonalization, we say it is \emph{diagonalizable}. When discussing eigenvalues of a matrix, we give the following conventional ordering to the eigenvalues: $\mM$ has eigenvalues \[\lam_1 \geq \lam_2 \geq \cdots, \geq \lam_{n-1} \geq \lam_n.\]

The following theorem guarantees that adjacency matrices of undirected graphs always have a nice diagonalization:
\begin{theorem}\label{thm:symmetric-diagonalizable}
  Let $\mM \in \mathbb{R}^{n \times n}$ be a symmetric. Then $\mM$ has $n$ linearly independent eigenvectors $\vv_j$, and so we can express $\mM = \mV \mLam \mV\inv$.
\end{theorem}
Since undirected graphs (and, in particular, standard graphs) have symmetric adjacency matrices $\mA$, we can always make use of the diagonalization of $\mA$ and $\mL$.

Recall that the \emph{rank} of a matrix is equal to (1) the dimension of the columnspace of $\mM$, (2) the dimension of the rowspace of $\mM$.
\begin{proposition}
  If $\mM$ is a diagonalizable, then $\rank(\mM)$ is equal to the number of nonzero eigenvalues $\lam_j \neq 0$ of $\mM$, which equals the number of linearly independent eigenvectors $\vv_j$ of $\mM$.
\end{proposition}

\begin{proposition}\label{thm:rank-invertible}
  An $n\times n$ matrix $\mM$ is invertible iff $\rank(\mM) = n$.
\end{proposition}

An eigenvalue $\lambda$ of a matrix $\mM$ can have multiple linearly independent eigenvectors associated with it. The most straightforward example is the $n \times n$ identity matrix $\mI$, which has the eigenvalue 1 with $n$ different linearly independent eigenvectors; for exaxmple, each standard basis vector $\ve_j$ is an eigenvector of $\mI$, since $\ve_j = \mI\ve_j$.
If an eigenvalue $\lam$ of $\mM$ has multiple lienarly independent eigenvectors, then those eigenvectors generate a subspace, called an \emph{eigenspace} associated with the eigenvalue $\lam$. For Egienvalue $\lam_j$, let $E_j$ denote the subspace of all eigenvectors $\vv$ with eigenvalue $\lam_j$, i.e. all vectors $\vv$ such that $\mM\vv = \lam_j \vv$. Then the dimension of the subspace $E_j$ is defined to be the \emph{geometric multiplicity of the eigenvalue} $\lam_j$. If a matrix has 0 as an eigenvalue, this is so important that we give a special name to the multiplicity of the eigenvalue 0 -- if a matrix $\mM$ has eigenvalue 0 with geometric multiplicity $m_0$, then we say $\mM$ has \emph{nullity} $m_0$, which we will denote $\nullity(\mM)$.
This brings us to the ``Rank-nullity theorem", which has a useful connection to Proposition~\ref{thm:rank-invertible}:
\begin{theorem}[Rank-nullity]
  For any $\mM \in \mathbb{R}^{n \times n}$ the rank and nullity of $\mM$ satisfy $\rank(\mM) = n - \nullity(\mM)$. In particular, $\mM$ is invertible iff $\nullity(\mM) = 0$, i.e. 0 is not an eigenvalue of $\mM$.
\end{theorem}

Finally we connect these concepts to our analysis of networks, by looking more closely at Exercise~\ref{sec:notation:problem-laplacian-singular} from the previous section.
First, we restate the exercise as a result:
\begin{proposition}
Let $\mL$ be the Laplacian of a standard graph. Then $\mL$ has eigenvalue $\lam = 0$ with multiplicity at least 1, and $\ve$ as an eigenvector: $\mL\ve = 0\ve$.
\end{proposition}
This means Laplacian matrices are never invertible, since they always have nullity at least 1. However, something neat happens if we look more closely at this result.
But we'll get there -- first, we need one more preliminary result.
\begin{lemma}
  Let $G$ be a standard graph with $k$ connected components, $G_1, \cdots, G_k$, with connected component $G_j$ having Laplacian matrix $\mL_j$. Then the graph $G$ can have its nodes ordered so that the Laplacian matrix $\mL$ of $G$ is
  \[
\mL = \diag( \mL_1, \cdots, \mL_k ) = \bmat{\mL_1 & & \\  & \ddots &  \\  &  &  \mL_k}.
  \]
\end{lemma}
This lemma makes it easier to prove the following theorem, which gives us a first glimpse of how Linear Algebra can provide information useful to Graph Theory:
\begin{theorem}
  Let $G$ be a standard graph with $k$ connected components, and with Laplacian matrix $\mL$. Then $k = \nullity(\mL)$.
\end{theorem}
In words, the multiplicity of zero as an eigenvalue of $G$'s Laplacian matrix is the same as the number of distinct connected components of $G$.
